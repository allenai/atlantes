# Atlantes Architecture Overview

For specifics on different components, see the readmes in `ais/docs`.
## Model Development Pipeline

The typical workflow for developing an AIS model is as follows:

1. Generate Raw Trajectory Dataset
    - Dynamic and Static AIS data are merged and stored in a GCS bucket in parquet files (older files may still be in csv format)
    - Each parquet file contains all the AIS messages in a single trajectory for a given month
    - trajectories are uniquely identified by a trackId that is generated by the skylight platform
    - The trajectory parquet files are generally stored in a GCS bucket in the following format:
        - `year/{dataset_name}/{ais_type}/{int}/{flag_code}/{trackId}/{{trackId}_{month}.parquet`
    - Another Copy of this data is in Weka for Training on Ai2 on-prem hardware

    - ***Metadata*** is stored in GCS in a metadata parquet to support easily finding files based on metadata

2. Generate Labels for the Specific Tasks
    - Human Annotated AIS data is stored in Elastic Search and then exported to a gcs bucket in csv format
    - Machine Annotated AIS data (annotations based on metadata from static ais messages) is generated programmatically
    based on the specific task.
    - Run a dataset creation script that creates a file that link the paths in gcs to the labels
3. Train the model on the dataset
    - Training is launched via a script (gcp vm) or a beaker config file pointing to that script
    - The experiment is configured based on a yaml file for that experiment
    - Experiments and this configuration are tracked on wandb
4. Deployment to Integration Environment
    - To push a new model to integration we must complete the following steps:
        - Update Inference config to match model configuration
        - Point to new model checkpoint in `pipeline.py`
        - Merge to develop (Currently in flux as we are separating dependencies from [skylight-subpath-service](https://github.com/VulcanSkylight/skylight-subpath-service))

## Inference Pipeline

1. Ingest AIS data and run changepoint detection to creat subpaths every time we have detected the behavior of the vessel may have changed.
    - Changepoint Detection Allows us to modulate how often we run inference
2. Run entity classification service
    - Note that the entity model is stateful but the state management is handled in the subpath service repo.
3. If the entity transmitting its location is a vessel run activity classification and return the activity predicitions

For a more in depth discussion of how this fits into the rest of skylight see [Project Atlantes](https://allenai.atlassian.net/wiki/spaces/IUU/pages/29816291329/2024+Skylight+Technology+Roadmap)
